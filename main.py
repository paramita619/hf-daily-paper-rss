"""
ğŸš€ ç»ˆææ™ºèƒ½RSSèšåˆå™¨ v3.0 (GitHub Actions ç‰ˆ)
============================
æ ¸å¿ƒå‡çº§ï¼š
1. è¯­ä¹‰ç†è§£ï¼šå…³é”®è¯ç»„åˆ + ä¸Šä¸‹æ–‡åˆ†æ
2. æ™ºèƒ½å»é‡ï¼šæ ‡é¢˜ç›¸ä¼¼åº¦ + å†…å®¹æŒ‡çº¹
3. æ—¶æ•ˆæ€§è¡°å‡ï¼š24hå†…æœ€æ–°ï¼Œè¶…è¿‡48hé™æƒ
4. è´¨é‡ä¿¡å·èšåˆï¼šå¤šç»´å¼±ä¿¡å· â†’ å¼ºåˆ¤æ–­
5. è‡ªé€‚åº”é˜ˆå€¼ï¼šæ ¹æ®å½“æ—¥è´¨é‡åŠ¨æ€è°ƒæ•´
"""

import requests
import datetime
import PyRSS2Gen
from bs4 import BeautifulSoup
import re
import time
import xml.etree.ElementTree as ET
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
from difflib import SequenceMatcher
from collections import defaultdict
import hashlib
import json
import email.utils

# ================= ğŸ§  æ ¸å¿ƒçŸ¥è¯†åº“ =================

# åˆ†çº§æƒå¨åº“ï¼ˆå½±å“åŠ›é€’å‡ï¼‰
TIER_S_AUTHORITIES = {
    # å›¾çµå¥–çº§åˆ« + ç°å½¹é¡¶çº§å†³ç­–è€…
    "geoffrey hinton", "yann lecun", "yoshua bengio", "demis hassabis",
    "ilya sutskever", "sam altman", "dario amodei", "jensen huang",
    "satya nadella", "sundar pichai"
}

TIER_A_AUTHORITIES = {
    # é¡¶çº§ç ”ç©¶è€… + çŸ¥åå®éªŒå®¤è´Ÿè´£äºº
    "andrej karpathy", "andrew ng", "fei-fei li", "jeff dean",
    "franÃ§ois chollet", "pieter abbeel", "chelsea finn", "kaiming he",
    "openai", "deepmind", "anthropic", "meta ai", "google brain",
    "stanford ai lab", "berkeley ai research", "mit csail"
}

TIER_B_AUTHORITIES = {
    # çŸ¥åä½†éé¡¶çº§
    "sebastian ruder", "jeremy howard", "rachel thomas", "chris olah",
    "google research", "microsoft research", "apple machine learning",
    "cmu", "eth zurich", "tsinghua", "peking university"
}

# æŠ€æœ¯å…³é”®è¯ï¼šä¸‰å±‚åˆ†çº§
CORE_TECH_L1 = {
    # ç«¯ä¾§AIæ ¸å¿ƒï¼ˆæœ€é«˜ä»·å€¼ï¼‰
    "on-device ai", "on-device", "edge ai", "edge inference", "tinyml",
    "local llm", "local ai", "neural engine", "npu", "tpu acceleration",
    "int4", "int8", "4-bit quant", "8-bit quant", "gguf", "ggml",
    "llama.cpp", "mlx", "executorch", "coreml tools", "nnapi",
    "model compression", "neural compression"
}

CORE_TECH_L2 = {
    # åº•å±‚ä¼˜åŒ–ï¼ˆé«˜ä»·å€¼ï¼‰
    "quantization", "pruning", "distillation", "knowledge distillation",
    "tensorrt", "tflite", "onnxruntime", "openvino", "webgpu",
    "wasm ai", "metal performance", "cuda kernel", "triton compiler",
    "flash attention", "paged attention", "kv cache optimization"
}

CORE_TECH_L3 = {
    # ç›¸å…³æŠ€æœ¯ï¼ˆä¸­ä»·å€¼ï¼‰
    "transformer", "diffusion", "rag", "retrieval", "lora", "qlora",
    "peft", "adapter", "prefix tuning", "prompt tuning",
    "moe", "mixture of experts", "sparse model"
}

# ç¡¬ä»¶å…³é”®è¯ï¼ˆå¿…é¡»é…åˆæŠ€æœ¯è¯ï¼‰
HARDWARE_TERMS = {
    "a18 pro", "a18 bionic", "a17 pro", "m4 chip", "m4 pro", "m4 max",
    "snapdragon 8 elite", "snapdragon 8 gen 3", "dimensity 9400",
    "google tensor", "exynos", "h100", "h200", "b100", "b200", "blackwell",
    "apple silicon", "arm mali", "qualcomm hexagon"
}

# å…¬å¸/äº§å“ï¼ˆä½åŸºç¡€åˆ†ï¼‰
COMPANIES = {
    "apple", "google", "samsung", "qualcomm", "mediatek", "nvidia",
    "amd", "intel", "arm", "huawei", "xiaomi", "meta", "microsoft",
    "openai", "anthropic", "mistral", "cohere"
}

# åŠ¨ä½œè¯ï¼ˆçœŸå®å‘å¸ƒ vs ç‚’ä½œï¼‰
STRONG_ACTIONS = {
    "release", "released", "launch", "launched", "ship", "shipped",
    "announce", "announced", "unveil", "unveiled", "available now",
    "open source", "open-source", "publish", "published"
}

WEAK_ACTIONS = {
    "preview", "beta", "demo", "prototype", "concept", "teaser",
    "coming soon", "will launch", "plans to", "expected to"
}

# å™ªéŸ³è¯ï¼ˆåˆ†çº§æƒ©ç½šï¼‰
HARD_NOISE = {
    # é‡‘è/å•†ä¸š
    "stock price", "share price", "market cap", "quarterly earnings",
    "revenue beat", "profit margin", "dividend", "ipo", "acquisition deal",
    # æ¶ˆè´¹/ä¿ƒé”€
    "best deal", "discount code", "price drop", "coupon", "sale price",
    "limited time", "special offer",
    # å¤–è®¾/é…ä»¶
    "phone case", "screen protector", "wallpaper pack", "theme",
    "charging cable", "earbuds", "airpods case"
}

MEDIUM_NOISE = {
    # è°£è¨€/ç‚’ä½œ
    "rumor", "leak", "alleged", "reportedly", "sources say",
    "insider claims", "render", "mockup", "concept art",
    # æµ…å±‚å†…å®¹
    "top 10", "best of", "ranking", "comparison", "vs battle",
    "tier list", "listicle"
}

SOFT_NOISE = {
    # æ¡ä»¶æ€§å™ªéŸ³ï¼ˆå¦‚æœæ²¡æœ‰ç¡¬æ ¸æŠ€æœ¯ï¼Œåˆ™æ˜¯å™ªéŸ³ï¼‰
    "review", "hands-on", "unboxing", "first look", "impressions",
    "gameplay", "benchmark", "speed test"
}

# æƒå¨åŸŸåï¼ˆä¸‰çº§ä¿¡ä»»ï¼‰
TIER_S_DOMAINS = {
    # å­¦æœ¯/å®˜æ–¹
    "arxiv.org", "openreview.net", "ieeexplore.ieee.org", "dl.acm.org",
    "nature.com", "science.org", "pnas.org",
    # é¡¶çº§æœºæ„å®˜ç½‘
    "openai.com", "anthropic.com", "deepmind.google", "ai.meta.com",
    "research.google", "machinelearning.apple.com", "developer.apple.com",
    "pytorch.org", "tensorflow.org", "huggingface.co"
}

TIER_A_DOMAINS = {
    # çŸ¥åç§‘æŠ€åª’ä½“æ·±åº¦æŠ¥é“
    "techcrunch.com", "theverge.com", "arstechnica.com", "wired.com",
    # VC/æ™ºåº“
    "a16z.com", "sequoiacap.com", "ycombinator.com",
    # å¼€å‘è€…å¹³å°
    "github.com", "developer.nvidia.com", "developer.qualcomm.com"
}

TIER_B_DOMAINS = {
    # ç¤¾åŒº/èšåˆ
    "news.ycombinator.com", "reddit.com", "medium.com"
}

BLOCKED_DOMAINS = {
    "pinterest.com", "facebook.com", "instagram.com", "tiktok.com",
    "clickbait.com", "viralthread.com"
}

# ================= ğŸ› ï¸ å·¥å…·å‡½æ•° =================

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤å¤šä½™ç©ºç™½"""
    return re.sub(r"\s+", " ", (text or "")).strip()

def normalize_url(url: str) -> str:
    """æ ‡å‡†åŒ–URLï¼Œå»é™¤è¿½è¸ªå‚æ•°"""
    try:
        p = urlparse(url)
        # ç§»é™¤UTMå’Œå¸¸è§è¿½è¸ªå‚æ•°
        q = [(k, v) for (k, v) in parse_qsl(p.query, keep_blank_values=True)
             if not any(k.lower().startswith(prefix) for prefix in ["utm_", "fb", "tw", "ig"])
             and k.lower() not in {"ref", "source", "feature", "campaign", "medium"}]
        return urlunparse((p.scheme, p.netloc, p.path, p.params, urlencode(q, doseq=True), ""))
    except:
        return url

def get_domain(url: str) -> str:
    """æå–åŸŸå"""
    try:
        domain = urlparse(url).netloc.lower()
        return domain.replace("www.", "")
    except:
        return ""

def text_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

def content_fingerprint(title: str, desc: str = "") -> str:
    """ç”Ÿæˆå†…å®¹æŒ‡çº¹ï¼ˆç”¨äºå»é‡ï¼‰"""
    # æå–å…³é”®è¯ï¼Œå¿½ç•¥åœç”¨è¯
    stop_words = {"the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by"}
    words = re.findall(r'\w+', (title + " " + desc).lower())
    key_words = [w for w in words if w not in stop_words and len(w) > 3]
    return hashlib.md5(" ".join(sorted(key_words[:15])).encode()).hexdigest()

def parse_date_from_arxiv(link: str) -> datetime.datetime:
    """ä»arXivé“¾æ¥æå–æäº¤æ—¥æœŸ"""
    # arXivæ ¼å¼: https://arxiv.org/abs/YYMM.NNNNN
    match = re.search(r'/(\d{4})\.', link)
    if match:
        yymm = match.group(1)
        year = 2000 + int(yymm[:2])
        month = int(yymm[2:])
        return datetime.datetime(year, month, 1)
    return datetime.datetime.now()

def time_decay_factor(pub_date: datetime.datetime) -> float:
    """æ—¶æ•ˆæ€§è¡°å‡å› å­ï¼ˆ0.5-1.0ï¼‰"""
    now = datetime.datetime.now()
    hours_old = (now - pub_date).total_seconds() / 3600
    
    if hours_old < 24:
        return 1.0  # 24å°æ—¶å†…ï¼šæ»¡åˆ†
    elif hours_old < 48:
        return 0.9  # 48å°æ—¶å†…ï¼š0.9
    elif hours_old < 168:  # 1å‘¨
        return 0.8
    else:
        return 0.7  # 1å‘¨ä»¥ä¸Šï¼š0.7

# ================= ğŸ§  è¯­ä¹‰åˆ†æå¼•æ“ =================

class SemanticAnalyzer:
    """è¯­ä¹‰åˆ†æï¼šç†è§£å…³é”®è¯ç»„åˆå’Œä¸Šä¸‹æ–‡"""
    
    @staticmethod
    def detect_tech_combos(text: str) -> list:
        """æ£€æµ‹æŠ€æœ¯ç»„åˆï¼ˆæ¯”å•ä¸ªå…³é”®è¯æ›´å¼ºï¼‰"""
        text_lower = text.lower()
        combos = []
        
        # ç«¯ä¾§ + ä¼˜åŒ– = æ ¸å¿ƒä¸»é¢˜
        if any(t in text_lower for t in ["on-device", "edge", "local", "mobile"]):
            if any(t in text_lower for t in ["quantization", "compression", "optimization", "pruning"]):
                combos.append(("edge_optimization", 60))
        
        # ç¡¬ä»¶ + åŠ é€Ÿ = çœŸå®æ€§èƒ½
        if any(h in text_lower for h in HARDWARE_TERMS):
            if any(t in text_lower for t in ["benchmark", "performance", "acceleration", "speedup"]):
                combos.append(("hardware_perf", 50))
        
        # å¼€æº + å·¥å…· = å®ç”¨ä»·å€¼
        if any(t in text_lower for t in ["open source", "open-source", "github release"]):
            if any(t in text_lower for t in ["tool", "library", "framework", "sdk"]):
                combos.append(("opensource_tool", 45))
        
        # è®ºæ–‡ + å®éªŒå®¤ = é¡¶çº§ç ”ç©¶
        if any(t in text_lower for t in ["paper", "arxiv", "research"]):
            if any(lab in text_lower for lab in TIER_S_AUTHORITIES | TIER_A_AUTHORITIES):
                combos.append(("top_research", 80))
        
        # å…¬å¸ + å¼ºåŠ¨ä½œ = çœŸå‘å¸ƒï¼ˆéç‚’ä½œï¼‰
        if any(c in text_lower for c in COMPANIES):
            if any(a in text_lower for a in STRONG_ACTIONS):
                combos.append(("real_release", 35))
        
        return combos
    
    @staticmethod
    def detect_negative_combos(text: str) -> list:
        """æ£€æµ‹è´Ÿé¢ç»„åˆ"""
        text_lower = text.lower()
        negatives = []
        
        # å…¬å¸ + è°£è¨€ = ç‚’ä½œ
        if any(c in text_lower for c in COMPANIES):
            if any(n in text_lower for n in MEDIUM_NOISE):
                negatives.append(("company_rumor", -40))
        
        # è¯„æµ‹ + æ— æŠ€æœ¯ = æµ…å±‚
        if any(s in text_lower for s in SOFT_NOISE):
            has_tech = any(t in text_lower for t in CORE_TECH_L1 | CORE_TECH_L2)
            if not has_tech:
                negatives.append(("shallow_review", -35))
        
        # æ¦œå• + èšåˆ = ä½è´¨é‡
        if any(t in text_lower for t in ["top", "best", "ranking"]):
            if any(t in text_lower for t in ["apps", "tools", "services"]):
                negatives.append(("listicle", -30))
        
        return negatives
    
    @staticmethod
    def context_score(text: str) -> tuple:
        """ä¸Šä¸‹æ–‡ç»¼åˆè¯„åˆ†"""
        positive_combos = SemanticAnalyzer.detect_tech_combos(text)
        negative_combos = SemanticAnalyzer.detect_negative_combos(text)
        
        score = sum(s for _, s in positive_combos) + sum(s for _, s in negative_combos)
        reasons = [name for name, _ in positive_combos + negative_combos]
        
        return score, reasons

# ================= ğŸ¯ é«˜çº§è¯„åˆ†å¼•æ“ =================

class AdvancedScorer:
    """å¤šç»´åº¦è¯„åˆ†ç³»ç»Ÿ"""
    
    @staticmethod
    def score_authority(text: str) -> tuple:
        """æƒå¨è¯„åˆ†"""
        score = 0
        reasons = []
        
        text_lower = text.lower()
        
        # Sçº§æƒå¨ï¼š+100
        for auth in TIER_S_AUTHORITIES:
            if auth in text_lower:
                score += 100
                reasons.append(f"ğŸ†S-Authority:{auth}(+100)")
        
        # Açº§æƒå¨ï¼š+70
        for auth in TIER_A_AUTHORITIES:
            if auth in text_lower:
                score += 70
                reasons.append(f"â­A-Authority:{auth}(+70)")
        
        # Bçº§æƒå¨ï¼š+40
        for auth in TIER_B_AUTHORITIES:
            if auth in text_lower:
                score += 40
                reasons.append(f"âœ“B-Authority:{auth}(+40)")
        
        return score, reasons
    
    @staticmethod
    def score_technical_depth(text: str) -> tuple:
        """æŠ€æœ¯æ·±åº¦è¯„åˆ†ï¼ˆé€’å‡ç­–ç•¥ï¼‰"""
        score = 0
        reasons = []
        text_lower = text.lower()
        
        # L1æŠ€æœ¯ï¼šæ¯ä¸ª+40ï¼Œæœ€å¤š3ä¸ªï¼Œä¹‹åé€’å‡
        l1_count = 0
        for tech in CORE_TECH_L1:
            if tech in text_lower:
                l1_count += 1
                points = 40 if l1_count <= 3 else 10
                score += points
                reasons.append(f"L1-Tech:{tech}(+{points})")
        
        # L2æŠ€æœ¯ï¼šæ¯ä¸ª+25ï¼Œæœ€å¤š2ä¸ª
        l2_count = 0
        for tech in CORE_TECH_L2:
            if tech in text_lower:
                l2_count += 1
                if l2_count <= 2:
                    score += 25
                    reasons.append(f"L2-Tech:{tech}(+25)")
        
        # L3æŠ€æœ¯ï¼šæ¯ä¸ª+15ï¼Œæœ€å¤š2ä¸ª
        l3_count = 0
        for tech in CORE_TECH_L3:
            if tech in text_lower:
                l3_count += 1
                if l3_count <= 2:
                    score += 15
                    reasons.append(f"L3-Tech:{tech}(+15)")
        
        return score, reasons
    
    @staticmethod
    def score_domain_trust(url: str) -> tuple:
        """åŸŸåä¿¡ä»»è¯„åˆ†"""
        domain = get_domain(url)
        
        if domain in BLOCKED_DOMAINS:
            return -200, ["âŒBlocked-Domain"]
        
        if domain in TIER_S_DOMAINS or any(domain.endswith(f".{d}") for d in TIER_S_DOMAINS):
            return 50, [f"ğŸ”’S-Domain:{domain}(+50)"]
        
        if domain in TIER_A_DOMAINS or any(domain.endswith(f".{d}") for d in TIER_A_DOMAINS):
            return 30, [f"âœ“A-Domain:{domain}(+30)"]
        
        if domain in TIER_B_DOMAINS:
            return 15, [f"B-Domain:{domain}(+15)"]
        
        return 0, []
    
    @staticmethod
    def score_noise(text: str) -> tuple:
        """å™ªéŸ³æ£€æµ‹ï¼ˆåˆ†çº§æƒ©ç½šï¼‰"""
        text_lower = text.lower()
        
        # ç¡¬å™ªéŸ³ï¼šä¸€ç¥¨å¦å†³
        for noise in HARD_NOISE:
            if noise in text_lower:
                return -500, [f"âŒHardNoise:{noise}"]
        
        # ä¸­ç­‰å™ªéŸ³ï¼šå¤šä¸ªç´¯ç§¯
        medium_hits = sum(1 for noise in MEDIUM_NOISE if noise in text_lower)
        if medium_hits >= 2:
            return -300, [f"âŒMediumNoise:hits={medium_hits}"]
        elif medium_hits == 1:
            return -50, ["âš ï¸MediumNoise:1hit"]
        
        # è½¯å™ªéŸ³ï¼šéœ€è¦ä¸Šä¸‹æ–‡åˆ¤æ–­
        soft_hits = [noise for noise in SOFT_NOISE if noise in text_lower]
        if soft_hits:
            has_hardcore = any(tech in text_lower for tech in CORE_TECH_L1 | CORE_TECH_L2)
            if not has_hardcore:
                return -150, [f"âŒSoftNoise:{soft_hits[0]}-no-tech"]
            else:
                return -10, [f"âš ï¸SoftNoise:{soft_hits[0]}-with-tech"]
        
        return 0, []
    
    @staticmethod
    def score_industry_gate(text: str) -> tuple:
        """è¡Œä¸šè¯é—¨æ§ï¼ˆå¿…é¡»é…åˆå®è´¨å†…å®¹ï¼‰"""
        text_lower = text.lower()
        
        has_company = any(c in text_lower for c in COMPANIES)
        has_hardware = any(h in text_lower for h in HARDWARE_TERMS)
        has_tech = any(t in text_lower for t in CORE_TECH_L1 | CORE_TECH_L2 | CORE_TECH_L3)
        has_strong_action = any(a in text_lower for a in STRONG_ACTIONS)
        
        if has_company or has_hardware:
            if has_tech or has_strong_action:
                return 20, ["âœ“Industry-Gate:pass(+20)"]
            else:
                return -30, ["âŒIndustry-Gate:fail(-30)"]
        
        return 0, []
    
    @staticmethod
    def comprehensive_score(title: str, desc: str, url: str, source: str, pub_date: datetime.datetime = None) -> tuple:
        """ç»¼åˆè¯„åˆ†"""
        text = f"{title} {desc}"
        total_score = 0
        all_reasons = []
        
        # 1. æ¥æºåŸºç¡€åˆ†
        source_base = {
            "HF Papers": 25,
            "AlphaXiv": 25,
            "TechCrunch": 15,
            "a16z": 20,
            "Hacker News": 10
        }.get(source, 10)
        total_score += source_base
        all_reasons.append(f"Source:{source}(+{source_base})")
        
        # 2. åŸŸåä¿¡ä»»
        domain_score, domain_reasons = AdvancedScorer.score_domain_trust(url)
        total_score += domain_score
        all_reasons.extend(domain_reasons)
        
        # 3. å™ªéŸ³æ£€æµ‹ï¼ˆå¯èƒ½ä¸€ç¥¨å¦å†³ï¼‰
        noise_score, noise_reasons = AdvancedScorer.score_noise(text)
        if noise_score <= -300:
            return noise_score, noise_reasons  # ç«‹å³è¿”å›
        total_score += noise_score
        all_reasons.extend(noise_reasons)
        
        # 4. æƒå¨è¯„åˆ†
        auth_score, auth_reasons = AdvancedScorer.score_authority(text)
        total_score += auth_score
        all_reasons.extend(auth_reasons)
        
        # 5. æŠ€æœ¯æ·±åº¦
        tech_score, tech_reasons = AdvancedScorer.score_technical_depth(text)
        total_score += tech_score
        all_reasons.extend(tech_reasons)
        
        # 6. è¡Œä¸šé—¨æ§
        gate_score, gate_reasons = AdvancedScorer.score_industry_gate(text)
        total_score += gate_score
        all_reasons.extend(gate_reasons)
        
        # 7. è¯­ä¹‰åˆ†æ
        context_score, context_reasons = SemanticAnalyzer.context_score(text)
        total_score += context_score
        all_reasons.extend(context_reasons)
        
        # 8. æ—¶æ•ˆæ€§è¡°å‡
        if pub_date:
            decay = time_decay_factor(pub_date)
            total_score = int(total_score * decay)
            if decay < 1.0:
                all_reasons.append(f"TimeFactor:Ã—{decay:.1f}")
        
        return total_score, all_reasons

# ================= ğŸ”„ æ™ºèƒ½å»é‡å™¨ =================

class SmartDeduplicator:
    """æ™ºèƒ½å»é‡ï¼šä¸ä»…çœ‹URLï¼Œè¿˜çœ‹å†…å®¹ç›¸ä¼¼åº¦"""
    
    @staticmethod
    def deduplicate(items: list) -> list:
        """å»é‡é€»è¾‘"""
        seen_urls = set()
        seen_fingerprints = defaultdict(list)
        unique_items = []
        
        for item in items:
            url = item.get("link", "")
            
            # 1. URLå»é‡
            if url in seen_urls:
                continue
            
            # 2. å†…å®¹æŒ‡çº¹å»é‡
            fingerprint = content_fingerprint(item.get("title", ""), item.get("desc", ""))
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é«˜åº¦ç›¸ä¼¼çš„å†…å®¹
            is_duplicate = False
            for existing in seen_fingerprints[fingerprint[:8]]:  # å‰8ä½ä½œä¸ºæ¡¶
                if text_similarity(item.get("title", ""), existing.get("title", "")) > 0.85:
                    # æ ‡é¢˜85%ç›¸ä¼¼ï¼Œåˆ¤å®šä¸ºé‡å¤
                    # ä¿ç•™åˆ†æ•°æ›´é«˜æˆ–æ¥æºæ›´æƒå¨çš„
                    if item.get("score", 0) > existing.get("score", 0):
                        # æ›¿æ¢æ—§çš„
                        unique_items.remove(existing)
                        seen_fingerprints[fingerprint[:8]].remove(existing)
                    else:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                seen_urls.add(url)
                seen_fingerprints[fingerprint[:8]].append(item)
                unique_items.append(item)
        
        return unique_items

# ================= ğŸ•·ï¸ æŠ“å–å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ =================

def fetch_huggingface():
    """HuggingFaceæ¯æ—¥è®ºæ–‡"""
    print("ğŸ“„ Fetching HuggingFace Papers...")
    try:
        resp = requests.get("https://huggingface.co/papers", headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }, timeout=20)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        
        articles = []
        for article in soup.find_all("article")[:25]:
            h3 = article.find("h3")
            a = article.find("a", href=True)
            if h3 and a:
                title = clean_text(h3.get_text())
                link = "https://huggingface.co" + a["href"] if a["href"].startswith("/") else a["href"]
                
                # å°è¯•æå–æè¿°
                desc_tag = article.find("p", class_="line-clamp-2")
                desc = clean_text(desc_tag.get_text()) if desc_tag else ""
                
                articles.append({
                    "title": title,
                    "link": normalize_url(link),
                    "source": "HF Papers",
                    "desc": desc,
                    "pub_date": datetime.datetime.now()
                })
        
        print(f"  âœ“ Found {len(articles)} papers")
        return articles
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []

def fetch_arxiv():
    """arXivæœ€æ–°è®ºæ–‡"""
    print("ğŸ“„ Fetching arXiv...")
    try:
        url = ("http://export.arxiv.org/api/query?"
               "search_query=cat:cs.AI+OR+cat:cs.CV+OR+cat:cs.CL+OR+cat:cs.LG"
               "&start=0&max_results=50&sortBy=submittedDate&sortOrder=descending")
        
        resp = requests.get(url, timeout=20)
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
        ns = {"atom": "http://www.w3.org/2005/Atom"}
        
        articles = []
        for entry in root.findall("atom:entry", ns):
            title_tag = entry.find("atom:title", ns)
            summary_tag = entry.find("atom:summary", ns)
            link_tag = entry.find("atom:id", ns)
            published_tag = entry.find("atom:published", ns)
            
            if title_tag is not None and link_tag is not None:
                title = clean_text(title_tag.text)
                summary = clean_text(summary_tag.text) if summary_tag is not None else ""
                link = link_tag.text
                
                # è§£æå‘å¸ƒæ—¥æœŸ
                pub_date = datetime.datetime.now()
                if published_tag is not None:
                    try:
                        pub_date = datetime.datetime.fromisoformat(published_tag.text.replace('Z', '+00:00'))
                    except:
                        pub_date = parse_date_from_arxiv(link)
                
                articles.append({
                    "title": title,
                    "link": normalize_url(link),
                    "source": "AlphaXiv",
                    "desc": summary[:400],
                    "pub_date": pub_date
                })
        
        print(f"  âœ“ Found {len(articles)} papers")
        return articles
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []

def fetch_hacker_news_smart():
    """HNæ™ºèƒ½æœç´¢ï¼ˆå…³é”®è¯è¿‡æ»¤ï¼‰"""
    print("ğŸ“° Fetching Hacker News (smart search)...")
    
    # ç²¾é€‰å…³é”®è¯ï¼ˆé«˜ä¿¡å·ï¼‰
    keywords = [
        "on-device ai", "edge ai", "local llm", "quantization",
        "llama.cpp", "mlx", "coreml", "onnxruntime", "executorch",
        "int4", "int8", "tinyml", "npu", "webgpu", "wasm"
    ]
    
    articles = []
    seen = set()
    
    try:
        for kw in keywords:
            url = f"https://hn.algolia.com/api/v1/search_by_date?query={requests.utils.quote(kw)}&tags=story&hitsPerPage=15"
            data = requests.get(url, timeout=15).json()
            
            for hit in data.get("hits", []):
                title = clean_text(hit.get("title", ""))
                link = hit.get("url", "")
                created_at = hit.get("created_at", "")
                
                if not title or not link:
                    continue
                
                link = normalize_url(link)
                if link in seen:
                    continue
                seen.add(link)
                
                # è§£ææ—¶é—´
                pub_date = datetime.datetime.now()
                if created_at:
                    try:
                        pub_date = datetime.datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    except:
                        pass
                
                articles.append({
                    "title": title,
                    "link": link,
                    "source": "Hacker News",
                    "desc": "",
                    "pub_date": pub_date
                })
            
            time.sleep(0.1)
        
        print(f"  âœ“ Found {len(articles)} items")
        return articles
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []

def fetch_techcrunch():
    """TechCrunch RSS"""
    print("ğŸ“° Fetching TechCrunch...")
    try:
        resp = requests.get(
            "https://techcrunch.com/category/artificial-intelligence/feed/",
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=20
        )
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
        
        articles = []
        for item in root.findall("./channel/item")[:20]:
            title = clean_text(item.findtext("title", ""))
            link = clean_text(item.findtext("link", ""))
            desc = clean_text(item.findtext("description", ""))
            pub_date_str = item.findtext("pubDate", "")
            
            # è§£ææ—¥æœŸ
            pub_date = datetime.datetime.now()
            if pub_date_str:
                try:
                    pub_date = email.utils.parsedate_to_datetime(pub_date_str)
                except:
                    pass
            
            if title and link:
                articles.append({
                    "title": title,
                    "link": normalize_url(link),
                    "source": "TechCrunch",
                    "desc": desc[:300],
                    "pub_date": pub_date
                })
        
        print(f"  âœ“ Found {len(articles)} articles")
        return articles
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []

def fetch_a16z():
    """a16z RSS"""
    print("ğŸ“° Fetching a16z...")
    try:
        resp = requests.get(
            "https://a16z.com/feed/",
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=20
        )
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
        
        articles = []
        for item in root.findall("./channel/item")[:20]:
            title = clean_text(item.findtext("title", ""))
            link = clean_text(item.findtext("link", ""))
            desc = clean_text(item.findtext("description", ""))
            
            if title and link:
                articles.append({
                    "title": title,
                    "link": normalize_url(link),
                    "source": "a16z",
                    "desc": desc[:300],
                    "pub_date": datetime.datetime.now()
                })
        
        print(f"  âœ“ Found {len(articles)} articles")
        return articles
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []

# ================= ğŸ¯ æ™ºèƒ½é€‰æ‹©å™¨ =================

class SmartSelector:
    """æ™ºèƒ½é€‰æ‹©ï¼šä¸æ˜¯ç®€å•çš„Top Nï¼Œè€Œæ˜¯å¤šæ ·æ€§+è´¨é‡å¹³è¡¡"""
    
    @staticmethod
    def select_top(items: list, max_total: int = 10, category_quota: dict = None) -> list:
        """é€‰æ‹©æœ€ä½³å†…å®¹"""
        if category_quota is None:
            category_quota = {
                "æ¨¡å‹ç®—æ³•": 4,
                "å¹³å°åº•åº§": 3,
                "è¡Œä¸šåŠ¨æ€": 2,
                "å¤§V/æƒå¨": 1
            }
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        by_category = defaultdict(list)
        for item in items:
            by_category[item["category"]].append(item)
        
        # æ¯ä¸ªç±»åˆ«å†…éƒ¨æŒ‰åˆ†æ•°æ’åº
        for cat in by_category:
            by_category[cat].sort(key=lambda x: x["score"], reverse=True)
        
        selected = []
        used_links = set()
        
        # 1. æŒ‰é…é¢é€‰æ‹©
        for cat, quota in category_quota.items():
            for item in by_category.get(cat, [])[:quota]:
                if item["link"] not in used_links:
                    selected.append(item)
                    used_links.add(item["link"])
        
        # 2. å‰©ä½™åé¢ï¼šä»æ‰€æœ‰ç±»åˆ«ä¸­é€‰æœ€é«˜åˆ†
        if len(selected) < max_total:
            all_remaining = [
                item for item in items
                if item["link"] not in used_links
            ]
            all_remaining.sort(key=lambda x: x["score"], reverse=True)
            
            for item in all_remaining:
                if len(selected) >= max_total:
                    break
                selected.append(item)
                used_links.add(item["link"])
        
        # 3. æœ€ç»ˆæŒ‰åˆ†æ•°æ’åº
        selected.sort(key=lambda x: x["score"], reverse=True)
        
        return selected[:max_total]

# ================= ğŸ“Š åˆ†ç±»å™¨ =================

def categorize(item: dict) -> str:
    """æ™ºèƒ½åˆ†ç±»"""
    text = f"{item.get('title', '')} {item.get('desc', '')}".lower()
    source = item.get("source", "")
    
    # 1. è®ºæ–‡æº â†’ æ¨¡å‹ç®—æ³•
    if source in ("HF Papers", "AlphaXiv"):
        return "æ¨¡å‹ç®—æ³•"
    
    # 2. æƒå¨ â†’ å¤§V/æƒå¨
    if any(auth in text for auth in TIER_S_AUTHORITIES | TIER_A_AUTHORITIES):
        return "å¤§V/æƒå¨"
    
    # 3. ç«¯ä¾§æŠ€æœ¯ â†’ å¹³å°åº•åº§
    if any(tech in text for tech in CORE_TECH_L1 | CORE_TECH_L2):
        return "å¹³å°åº•åº§"
    
    # 4. å…¬å¸+åŠ¨ä½œ â†’ è¡Œä¸šåŠ¨æ€
    if any(c in text for c in COMPANIES) and any(a in text for a in STRONG_ACTIONS):
        return "è¡Œä¸šåŠ¨æ€"
    
    # 5. é»˜è®¤
    return "è¡Œä¸šåŠ¨æ€"

# ================= ğŸš€ ä¸»ç¨‹åº =================

def main():
    print("\n" + "="*70)
    print("ğŸš€ ç»ˆææ™ºèƒ½RSSèšåˆå™¨ v3.0")
    print("="*70 + "\n")
    
    # 1. æŠ“å–
    print("ğŸ“¡ Fetching from all sources...\n")
    all_items = []
    
    all_items.extend(fetch_huggingface())
    all_items.extend(fetch_arxiv())
    all_items.extend(fetch_hacker_news_smart())
    all_items.extend(fetch_techcrunch
