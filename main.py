"""
ğŸ§  æ™ºèƒ½RSSèšåˆå™¨ v2.0
====================
æ ¸å¿ƒç†å¿µï¼š
1. å¤šç»´åº¦è¯„åˆ†ï¼šæ¥æºå¯ä¿¡åº¦ + å†…å®¹è´¨é‡ + å…³é”®è¯åŒ¹é…
2. åŠ¨æ€é˜ˆå€¼ï¼šä¸åŒæ¥æºä½¿ç”¨ä¸åŒæ ‡å‡†
3. ä¸Šä¸‹æ–‡ç†è§£ï¼šä¸åªçœ‹å…³é”®è¯ï¼Œè¿˜è¦çœ‹ç»„åˆå’Œä¸Šä¸‹æ–‡
4. è´Ÿé¢ä¿¡å·å¼ºåŒ–ï¼šåƒåœ¾å†…å®¹ä¸€ç¥¨å¦å†³
"""

import requests
import datetime
import PyRSS2Gen
from bs4 import BeautifulSoup
import re
import xml.etree.ElementTree as ET
from typing import List, Dict, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
import time
import os

# ================= ğŸ“Š æ•°æ®ç»“æ„å®šä¹‰ =================

class SourceTier(Enum):
    """æ¥æºç­‰çº§ï¼šå†³å®šåŸºç¡€ä¿¡ä»»åº¦"""
    TIER_S = 100  # é¡¶çº§ï¼šé¡¶ä¼šè®ºæ–‡ã€é¡¶çº§å®éªŒå®¤
    TIER_A = 80   # ä¸€æµï¼šçŸ¥åç§‘æŠ€åª’ä½“çš„æ·±åº¦æŠ¥é“
    TIER_B = 60   # è‰¯å¥½ï¼šè¡Œä¸šæ–°é—»ï¼Œä½†éœ€ä¸¥æ ¼ç­›é€‰
    TIER_C = 40   # ä¸€èˆ¬ï¼šèšåˆå¹³å°ï¼Œéœ€é…åˆå¼ºå…³é”®è¯


@dataclass
class Article:
    """æ–‡ç« æ•°æ®ç»“æ„"""
    title: str
    link: str
    source: str
    description: str = ""
    author: str = ""
    score: float = 0.0
    reasons: List[str] = field(default_factory=list)
    category: str = ""


# ================= ğŸ¯ æƒå¨çŸ¥è¯†åº“ =================

class AuthorityDatabase:
    """æƒå¨äººç‰©å’Œæœºæ„æ•°æ®åº“"""
    
    # å›¾çµå¥–å¾—ä¸»ã€é¢†åŸŸå¥ åŸºäººï¼ˆæ— è®ºè¯´ä»€ä¹ˆéƒ½å€¼å¾—å…³æ³¨ï¼‰
    PIONEERS = {
        "geoffrey hinton", "yann lecun", "yoshua bengio", "demis hassabis",
        "ilya sutskever", "andrej karpathy", "fei-fei li", "andrew ng",
        "jeff dean", "franÃ§ois chollet", "jÃ¼rgen schmidhuber", 
        "pieter abbeel", "chelsea finn", "kaiming he"
    }
    
    # é¡¶çº§ç ”ç©¶æœºæ„ï¼ˆè®ºæ–‡å¿…çœ‹ï¼‰
    TOP_LABS = {
        # å·¥ä¸šç•Œ
        "openai", "deepmind", "google brain", "google research", "meta ai", 
        "meta fair", "anthropic", "microsoft research", "apple ml research",
        "nvidia research", "stability ai",
        # å­¦æœ¯ç•Œ
        "mit csail", "stanford", "berkeley", "cmu", "princeton", 
        "eth zurich", "oxford", "cambridge", "tsinghua", "peking university"
    }
    
    # å½“å‰CEO/å…³é”®å†³ç­–è€…ï¼ˆé‡å¤§æˆ˜ç•¥åŠ¨æ€å€¼å¾—å…³æ³¨ï¼‰
    CURRENT_LEADERS = {
        "sam altman": "OpenAI CEO",
        "satya nadella": "Microsoft CEO", 
        "sundar pichai": "Google CEO",
        "jensen huang": "NVIDIA CEO",
        "dario amodei": "Anthropic CEO",
        "mark zuckerberg": "Meta CEO",
        "elon musk": "xAI CEO"
    }
    
    # çŸ¥åç ”ç©¶è€…ï¼ˆéœ€é…åˆæŠ€æœ¯å†…å®¹ï¼‰
    RESEARCHERS = {
        "sebastian ruder", "jeremy howard", "rachel thomas", 
        "chris olah", "distill pub", "eleuther ai", "laion",
        "hugging face team", "simran kaur", "alex krizhevsky"
    }
    
    @classmethod
    def check_authority(cls, text: str) -> Tuple[int, List[str]]:
        """
        æ£€æŸ¥æƒå¨æ€§
        è¿”å›ï¼š(åˆ†æ•°, åŒ¹é…çš„æƒå¨)
        """
        text_lower = text.lower()
        score = 0
        matched = []
        
        # å…ˆé©±è€…ï¼š+150åˆ†ï¼ˆå‡ ä¹ç¡®ä¿å…¥é€‰ï¼‰
        for pioneer in cls.PIONEERS:
            if pioneer in text_lower:
                score += 150
                matched.append(f"Pioneer: {pioneer.title()}")
        
        # é¡¶çº§å®éªŒå®¤ï¼š+120åˆ†
        for lab in cls.TOP_LABS:
            if lab in text_lower:
                score += 120
                matched.append(f"Top Lab: {lab.title()}")
        
        # ç°ä»»é¢†å¯¼è€…ï¼š+80åˆ†ï¼ˆä½†éœ€æ³¨æ„æ˜¯å¦ä¸ºå…«å¦æ–°é—»ï¼‰
        for leader, title in cls.CURRENT_LEADERS.items():
            if leader in text_lower:
                score += 80
                matched.append(f"Leader: {leader.title()} ({title})")
        
        # çŸ¥åç ”ç©¶è€…ï¼š+60åˆ†
        for researcher in cls.RESEARCHERS:
            if researcher in text_lower:
                score += 60
                matched.append(f"Researcher: {researcher.title()}")
        
        return score, matched


# ================= ğŸ”¬ æŠ€æœ¯å…³é”®è¯åº“ =================

class TechnicalKeywords:
    """æŠ€æœ¯å…³é”®è¯åˆ†çº§ç³»ç»Ÿ"""
    
    # æ ¸å¿ƒæŠ€æœ¯ï¼ˆç«¯ä¾§AI/åº•å±‚æŠ€æœ¯ï¼‰- é«˜ä»·å€¼
    HARDCORE_EDGE_AI = {
        # ç«¯ä¾§æ¨ç†
        "on-device ai", "edge ai", "tinyml", "mobile ai", "embedded ai",
        # ç¡¬ä»¶åŠ é€Ÿ
        "npu", "tpu", "neural engine", "tensor cores", "dsp acceleration",
        # æ¨¡å‹ä¼˜åŒ–
        "quantization", "pruning", "knowledge distillation", "model compression",
        "int4", "int8", "fp16", "bnb", "awq", "gptq", "gguf",
        # æ¡†æ¶/å·¥å…·
        "llama.cpp", "mlx", "executorch", "coreml", "tensorrt", "tflite", 
        "onnx runtime", "openvino",
        # å°æ¨¡å‹
        "slm", "small language model", "phi-", "gemma", "tinyllama", "mobilevlm"
    }
    
    # åº•å±‚æŠ€æœ¯ï¼ˆæ¶æ„/ç³»ç»Ÿï¼‰- é«˜ä»·å€¼
    INFRASTRUCTURE = {
        "cuda kernels", "triton", "gpu optimization", "distributed training",
        "moe", "mixture of experts", "flash attention", "paged attention",
        "kv cache", "speculative decoding", "continuous batching",
        "tensor parallelism", "pipeline parallelism"
    }
    
    # å‰æ²¿ç®—æ³•ï¼ˆæ¨¡å‹/è®­ç»ƒï¼‰- ä¸­é«˜ä»·å€¼
    ALGORITHMS = {
        "transformer", "attention mechanism", "diffusion model", "vae",
        "rlhf", "dpo", "constitutional ai", "chain-of-thought", "reasoning",
        "retrieval augmented", "rag", "fine-tuning", "lora", "qlora",
        "sparse autoencoders", "mechanistic interpretability"
    }
    
    # èŠ¯ç‰‡/ç¡¬ä»¶ - éœ€é…åˆæŠ€æœ¯å†…å®¹
    CHIPS = {
        "a18 pro", "a18 bionic", "m4 chip", "m4 pro", "m4 max",
        "snapdragon 8 elite", "snapdragon 8 gen", "dimensity 9400",
        "google tensor", "exynos", "h100", "h200", "b200", "blackwell"
    }
    
    # å…¬å¸/äº§å“ - ä½ä»·å€¼ï¼Œéœ€å¼ºæŠ€æœ¯è¯é…åˆ
    COMPANIES = {
        "apple", "google", "samsung", "qualcomm", "mediatek",
        "nvidia", "amd", "intel", "arm", "huawei", "xiaomi",
        "openai", "anthropic", "meta", "microsoft"
    }
    
    @classmethod
    def analyze_technical_depth(cls, text: str) -> Tuple[int, List[str]]:
        """
        åˆ†ææŠ€æœ¯æ·±åº¦
        è¿”å›ï¼š(åˆ†æ•°, åŒ¹é…çš„æŠ€æœ¯ç‚¹)
        """
        text_lower = text.lower()
        score = 0
        matched = []
        
        # æ ¸å¿ƒæŠ€æœ¯ï¼šæ¯ä¸ª+50åˆ†
        for tech in cls.HARDCORE_EDGE_AI:
            if tech in text_lower:
                score += 50
                matched.append(f"Edge AI: {tech}")
        
        # åº•å±‚æŠ€æœ¯ï¼šæ¯ä¸ª+45åˆ†
        for infra in cls.INFRASTRUCTURE:
            if infra in text_lower:
                score += 45
                matched.append(f"Infrastructure: {infra}")
        
        # ç®—æ³•ï¼šæ¯ä¸ª+35åˆ†
        for algo in cls.ALGORITHMS:
            if algo in text_lower:
                score += 35
                matched.append(f"Algorithm: {algo}")
        
        # èŠ¯ç‰‡ï¼šæ¯ä¸ª+20åˆ†ï¼ˆå¿…é¡»é…åˆå…¶ä»–æŠ€æœ¯è¯ï¼‰
        chip_count = sum(1 for chip in cls.CHIPS if chip in text_lower)
        if chip_count > 0:
            score += chip_count * 20
            matched.append(f"Hardware: {chip_count} chips mentioned")
        
        # å…¬å¸åï¼šæ¯ä¸ª+5åˆ†ï¼ˆåŸºç¡€åˆ†ï¼Œä¸å¤Ÿå…¥é€‰ï¼‰
        company_count = sum(1 for company in cls.COMPANIES if company in text_lower)
        if company_count > 0:
            score += company_count * 5
        
        return score, matched


# ================= ğŸ—‘ï¸ å™ªéŸ³è¿‡æ»¤å™¨ =================

class NoiseFilter:
    """åƒåœ¾å†…å®¹æ£€æµ‹å™¨"""
    
    # é‡‘è/å•†ä¸šæ–°é—»ï¼ˆé™¤éæ˜¯é‡å¤§æˆ˜ç•¥ï¼‰
    FINANCIAL_NOISE = {
        "stock price", "market cap", "quarterly earnings", "revenue beat",
        "shares surge", "dividend", "analyst rating", "price target",
        "stock split", "ipo"
    }
    
    # æ¶ˆè´¹è€…/è¯„æµ‹ï¼ˆé™¤éæ˜¯æŠ€æœ¯æ·±åº¦è¯„æµ‹ï¼‰
    CONSUMER_NOISE = {
        "best deal", "discount", "sale", "price drop", "coupon",
        "unboxing", "hands-on first look", "top 10 apps", "wallpaper",
        "case", "screen protector", "accessory", "color options"
    }
    
    # è°£è¨€/ç‚’ä½œ
    RUMOR_NOISE = {
        "rumor", "leak suggests", "allegedly", "insider claims",
        "render shows", "concept design", "mockup", "speculation"
    }
    
    # å¨±ä¹/ç¤¾äº¤
    ENTERTAINMENT_NOISE = {
        "meme", "viral", "tiktok trend", "instagram story",
        "celebrity", "influencer collab"
    }
    
    # ä½è´¨é‡èšåˆ
    AGGREGATION_NOISE = {
        "this week in", "daily roundup", "news digest",
        "what you missed", "5 things to know"
    }
    
    @classmethod
    def check_noise(cls, text: str) -> Tuple[bool, List[str]]:
        """
        æ£€æµ‹æ˜¯å¦ä¸ºå™ªéŸ³
        è¿”å›ï¼š(æ˜¯å¦ä¸ºå™ªéŸ³, å‘½ä¸­çš„å™ªéŸ³ç±»å‹)
        """
        text_lower = text.lower()
        noise_found = []
        
        # æ£€æŸ¥å„ç±»å™ªéŸ³
        for noise in cls.FINANCIAL_NOISE:
            if noise in text_lower:
                noise_found.append(f"Financial: {noise}")
        
        for noise in cls.CONSUMER_NOISE:
            if noise in text_lower:
                noise_found.append(f"Consumer: {noise}")
        
        for noise in cls.RUMOR_NOISE:
            if noise in text_lower:
                noise_found.append(f"Rumor: {noise}")
        
        for noise in cls.ENTERTAINMENT_NOISE:
            if noise in text_lower:
                noise_found.append(f"Entertainment: {noise}")
        
        for noise in cls.AGGREGATION_NOISE:
            if noise in text_lower:
                noise_found.append(f"Low-quality: {noise}")
        
        # å¦‚æœå‘½ä¸­2ä¸ªä»¥ä¸Šå™ªéŸ³å…³é”®è¯ï¼Œåˆ¤å®šä¸ºåƒåœ¾
        is_noise = len(noise_found) >= 2
        
        return is_noise, noise_found


# ================= ğŸ¯ æ™ºèƒ½è¯„åˆ†å¼•æ“ =================

class IntelligentScorer:
    """æ™ºèƒ½è¯„åˆ†ç³»ç»Ÿ"""
    
    # ä¸åŒæ¥æºçš„åŸºç¡€åˆ†å’Œé˜ˆå€¼
    SOURCE_CONFIG = {
        "HF Papers": {"base": 100, "tier": SourceTier.TIER_S, "threshold": 120},
        "AlphaXiv": {"base": 100, "tier": SourceTier.TIER_S, "threshold": 120},
        "Hacker News": {"base": 40, "tier": SourceTier.TIER_C, "threshold": 80},
        "TechCrunch": {"base": 60, "tier": SourceTier.TIER_B, "threshold": 100},
        "a16z": {"base": 80, "tier": SourceTier.TIER_A, "threshold": 100},
    }
    
    @classmethod
    def score_article(cls, article: Article) -> Tuple[float, List[str], bool]:
        """
        ç»¼åˆè¯„åˆ†
        è¿”å›ï¼š(æ€»åˆ†, è¯„åˆ†åŸå› , æ˜¯å¦é€šè¿‡)
        """
        text = f"{article.title} {article.description}"
        total_score = 0
        reasons = []
        
        # 1. æ¥æºåŸºç¡€åˆ†
        config = cls.SOURCE_CONFIG.get(article.source, {"base": 50, "threshold": 80})
        source_base = config["base"]
        threshold = config["threshold"]
        
        total_score += source_base
        reasons.append(f"Source base: +{source_base} ({article.source})")
        
        # 2. å™ªéŸ³æ£€æµ‹ï¼ˆä¸€ç¥¨å¦å†³ï¼‰
        is_noise, noise_reasons = NoiseFilter.check_noise(text)
        if is_noise:
            reasons.append(f"âŒ NOISE DETECTED: {', '.join(noise_reasons)}")
            return -100, reasons, False
        
        # 3. æƒå¨æ€§æ£€æŸ¥
        auth_score, auth_matches = AuthorityDatabase.check_authority(text)
        if auth_score > 0:
            total_score += auth_score
            reasons.extend(auth_matches)
        
        # 4. æŠ€æœ¯æ·±åº¦åˆ†æ
        tech_score, tech_matches = TechnicalKeywords.analyze_technical_depth(text)
        if tech_score > 0:
            total_score += tech_score
            reasons.extend(tech_matches)
        
        # 5. ç‰¹æ®ŠåŠ æˆï¼šæ ‡é¢˜åŒ…å«æŠ€æœ¯è¯+æƒå¨
        title_lower = article.title.lower()
        if any(tech in title_lower for tech in TechnicalKeywords.HARDCORE_EDGE_AI):
            if any(auth in title_lower for auth in AuthorityDatabase.TOP_LABS):
                total_score += 30
                reasons.append("Bonus: Tech+Authority in title")
        
        # 6. åˆ¤æ–­æ˜¯å¦é€šè¿‡
        passed = total_score >= threshold
        
        return total_score, reasons, passed
    
    @classmethod
    def categorize(cls, article: Article, reasons: List[str]) -> str:
        """æ™ºèƒ½åˆ†ç±»"""
        # ä»è¯„åˆ†åŸå› æ¨æ–­ç±»åˆ«
        reason_text = " ".join(reasons).lower()
        
        if "pioneer" in reason_text or "leader" in reason_text:
            return "ğŸ“ æƒå¨å‘å£°"
        elif "top lab" in reason_text:
            return "ğŸ”¬ é¡¶çº§ç ”ç©¶"
        elif "edge ai" in reason_text or "infrastructure" in reason_text:
            return "âš¡ ç«¯ä¾§/åº•å±‚æŠ€æœ¯"
        elif "algorithm" in reason_text:
            return "ğŸ§  æ¨¡å‹ç®—æ³•"
        elif "hardware" in reason_text:
            return "ğŸ’» èŠ¯ç‰‡ç¡¬ä»¶"
        else:
            return "ğŸ“° è¡Œä¸šåŠ¨æ€"


# ================= ğŸ•·ï¸ æ•°æ®æŠ“å–å™¨ =================

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬"""
    if not text: return ""
    return re.sub(r'\s+', ' ', text).strip()


def fetch_huggingface() -> List[Article]:
    """æŠ“å–Hugging Faceæ¯æ—¥è®ºæ–‡ï¼ˆé¡¶çº§æ¥æºï¼‰"""
    print("ğŸ“„ Fetching Hugging Face Papers...")
    try:
        resp = requests.get("https://huggingface.co/papers", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        articles = []
        
        for article in soup.find_all('article')[:20]:  # åªçœ‹å‰20ç¯‡
            h3 = article.find('h3')
            if h3:
                title = clean_text(h3.get_text())
                link_tag = article.find('a')
                if link_tag and 'href' in link_tag.attrs:
                    link = "https://huggingface.co" + link_tag['href']
                    
                    # å°è¯•æå–ä½œè€…ä¿¡æ¯
                    author_tag = article.find('div', class_='text-sm')
                    author = clean_text(author_tag.get_text()) if author_tag else ""
                    
                    articles.append(Article(
                        title=title,
                        link=link,
                        source="HF Papers",
                        author=author
                    ))
        
        print(f"  âœ“ Found {len(articles)} papers")
        return articles
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []


def fetch_arxiv() -> List[Article]:
    """æŠ“å–arXivæœ€æ–°è®ºæ–‡ï¼ˆé¡¶çº§æ¥æºï¼‰"""
    print("ğŸ“„ Fetching arXiv Papers...")
    try:
        # åªæŠ“å–AI/CV/CL/LGç›¸å…³çš„æœ€æ–°è®ºæ–‡
        url = "http://export.arxiv.org/api/query?search_query=cat:cs.AI+OR+cat:cs.CV+OR+cat:cs.CL+OR+cat:cs.LG&start=0&max_results=30&sortBy=submittedDate&sortOrder=descending"
        resp = requests.get(url, timeout=15)
        root = ET.fromstring(resp.content)
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        
        articles = []
        for entry in root.findall('atom:entry', ns):
            title_tag = entry.find('atom:title', ns)
            summary_tag = entry.find('atom:summary', ns)
            link_tag = entry.find('atom:id', ns)
            authors = entry.findall('atom:author', ns)
            
            if title_tag is not None and link_tag is not None:
                title = clean_text(title_tag.text)
                summary = clean_text(summary_tag.text) if summary_tag is not None else ""
                link = link_tag.text
                
                # æå–ç¬¬ä¸€ä½œè€…
                author = ""
                if authors:
                    author_name = authors[0].find('atom:name', ns)
                    if author_name is not None:
                        author = clean_text(author_name.text)
                
                articles.append(Article(
                    title=title,
                    link=link,
                    source="AlphaXiv",
                    description=summary[:300],
                    author=author
                ))
        
        print(f"  âœ“ Found {len(articles)} papers")
        return articles
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []


def fetch_hacker_news() -> List[Article]:
    """æŠ“å–Hacker Newsï¼ˆéœ€ä¸¥æ ¼ç­›é€‰ï¼‰"""
    print("ğŸ“° Fetching Hacker News...")
    try:
        # ä½¿ç”¨ Algolia API æœç´¢å…³é”®è¯ï¼Œæ¯”æŠ“å–é¦–é¡µæ›´ç²¾å‡†
        queries = ["edge ai", "on-device", "llm", "npu", "quantization", "apple intelligence"]
        articles = []
        seen_ids = set()
        
        for q in queries:
            try:
                url = f"https://hn.algolia.com/api/v1/search_by_date?query={q}&tags=story&hitsPerPage=10"
                resp = requests.get(url, timeout=5).json()
                
                for hit in resp.get('hits', []):
                    obj_id = hit.get('objectID')
                    if obj_id in seen_ids: continue
                    seen_ids.add(obj_id)
                    
                    title = clean_text(hit.get('title'))
                    link = hit.get('url') or f"https://news.ycombinator.com/item?id={obj_id}"
                    
                    if title:
                        articles.append(Article(
                            title=title,
                            link=link,
                            source="Hacker News"
                        ))
                time.sleep(0.1)
            except:
                continue
        
        print(f"  âœ“ Found {len(articles)} items")
        return articles
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []


def fetch_techcrunch() -> List[Article]:
    """æŠ“å–TechCrunch AIæ¿å—ï¼ˆéœ€ç­›é€‰ï¼‰"""
    print("ğŸ“° Fetching TechCrunch...")
    try:
        resp = requests.get(
            "https://techcrunch.com/category/artificial-intelligence/",
            headers=HEADERS,
            timeout=10
        )
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        articles = []
        # å°è¯•å¤šä¸ªå¯èƒ½çš„é€‰æ‹©å™¨
        for selector in ['.post-block__title a', '.loop-card__title a', 'h2 a', 'h3 a']:
            links = soup.select(selector)
            for link in links[:15]:  # æ¯ä¸ªé€‰æ‹©å™¨æœ€å¤š15ä¸ª
                title = clean_text(link.get_text())
                if len(title) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡é¢˜
                    articles.append(Article(
                        title=title,
                        link=link.get('href', ''),
                        source="TechCrunch"
                    ))
        
        # å»é‡
        seen = set()
        unique_articles = []
        for article in articles:
            if article.link and article.link not in seen:
                seen.add(article.link)
                unique_articles.append(article)
        
        print(f"  âœ“ Found {len(unique_articles)} articles")
        return unique_articles
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []


def fetch_a16z() -> List[Article]:
    """æŠ“å–a16zï¼ˆé¡¶çº§VCè§†è§’ï¼‰"""
    print("ğŸ“° Fetching a16z...")
    try:
        resp = requests.get("https://a16z.com/news-content/", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        articles = []
        for link in soup.find_all('a', href=True):
            title = clean_text(link.get_text())
            href = link['href']
            
            # ç®€å•çš„å¯å‘å¼è¿‡æ»¤
            if len(title) > 15 and "ai" in href.lower():
                if href.startswith('/'):
                    href = 'https://a16z.com' + href
                
                articles.append(Article(
                    title=title,
                    link=href,
                    source="a16z"
                ))
        
        # å»é‡
        seen = set()
        unique = []
        for a in articles:
            if a.link not in seen:
                seen.add(a.link)
                unique.append(a)
                
        print(f"  âœ“ Found {len(unique)} articles")
        return unique
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []


# ================= ğŸš€ ä¸»ç¨‹åº =================

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸ§  Intelligent RSS Aggregator v2.0")
    print("="*60 + "\n")
    
    # 1. æŠ“å–æ‰€æœ‰æ¥æº
    print("ğŸ“¡ Fetching from all sources...\n")
    all_articles = []
    
    all_articles.extend(fetch_huggingface())
    all_articles.extend(fetch_arxiv())
    all_articles.extend(fetch_hacker_news())
    all_articles.extend(fetch_techcrunch())
    all_articles.extend(fetch_a16z())
    
    print(f"\nğŸ“Š Total raw articles: {len(all_articles)}\n")
    print("="*60)
    print("ğŸ¯ Scoring and filtering...\n")
    
    # 2. å»é‡
    seen_links = set()
    unique_articles = []
    for article in all_articles:
        if article.link and article.link not in seen_links:
            seen_links.add(article.link)
            unique_articles.append(article)
    
    # 3. è¯„åˆ†å’Œç­›é€‰
    selected_articles = []
    rejected_count = 0
    
    for article in unique_articles:
        score, reasons, passed = IntelligentScorer.score_article(article)
        
        if passed:
            article.score = score
            article.reasons = reasons
            article.category = IntelligentScorer.categorize(article, reasons)
            selected_articles.append(article)
            
            print(f"âœ… [{score:.0f}] {article.category}")
            print(f"   {article.title[:80]}")
            print(f"   Source: {article.source}")
            if article.author:
                print(f"   Author: {article.author}")
            print(f"   Reasons: {reasons[0] if reasons else 'N/A'}")
            print()
        else:
            rejected_count += 1
            if score > 50:  # åªæ˜¾ç¤ºé«˜åˆ†ä½†æœªé€šè¿‡çš„ï¼ˆå¸®åŠ©è°ƒè¯•ï¼‰
                print(f"âŒ [{score:.0f}] {article.title[:60]}...")
                print(f"   Reason: {reasons[0] if reasons else 'Below threshold'}")
                print()
    
    # 4. æŒ‰åˆ†æ•°æ’åº
    selected_articles.sort(key=lambda x: x.score, reverse=True)
    
    # 5. ç”ŸæˆRSS
    print("="*60)
    print(f"ğŸ“ Generating RSS feed...\n")
    
    rss_items = []
    for article in selected_articles:
        # æ„å»ºæè¿°
        desc_parts = [
            f"<div style='font-family: Arial, sans-serif;'>",
            f"<p><strong>ğŸ“Š Quality Score: {article.score:.0f}</strong></p>",
            f"<p><strong>ğŸ“‚ Category:</strong> {article.category}</p>",
            f"<p><strong>ğŸ” Source:</strong> {article.source}</p>"
        ]
        
        if article.author:
            desc_parts.append(f"<p><strong>âœï¸ Author:</strong> {article.author}</p>")
        
        desc_parts.append(f"<p><strong>âœ¨ Why selected:</strong></p><ul>")
        for reason in article.reasons[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªåŸå› 
            desc_parts.append(f"<li>{reason}</li>")
        desc_parts.append("</ul>")
        
        if article.description:
            desc_parts.append(f"<p><strong>ğŸ“„ Summary:</strong> {article.description[:400]}...</p>")
        
        desc_parts.append("</div>")
        
        description = "\n".join(desc_parts)
        
        rss_items.append(PyRSS2Gen.RSSItem(
            title=f"[{article.score:.0f}] {article.category} | {article.title}",
            link=article.link,
            description=description,
            pubDate=datetime.datetime.now()
        ))
    
    # ç”ŸæˆRSSæ–‡ä»¶ (ä¿®æ”¹ä¸ºå½“å‰ç›®å½•ï¼Œé€‚é… GitHub Actions)
    rss = PyRSS2Gen.RSS2(
        title="ğŸ§  Intelligent AI & Tech Feed",
        link="https://github.com/paramita619/hf-daily-paper-rss",
        description="High-quality, authority-focused feed for AI research, edge computing, and technical breakthroughs. Powered by multi-dimensional scoring.",
        lastBuildDate=datetime.datetime.now(),
        items=rss_items
    )
    
    output_file = "edge_ai_daily.xml"  # ä¿®å¤åçš„æ–‡ä»¶å
    with open(output_file, "w", encoding='utf-8') as f:
        rss.write_xml(f)
    
    # 6. ç»Ÿè®¡æŠ¥å‘Š
    print("="*60)
    print("ğŸ“Š FINAL REPORT")
    print("="*60)
    print(f"Total articles fetched: {len(all_articles)}")
    print(f"Unique articles: {len(unique_articles)}")
    print(f"Articles passed filter: {len(selected_articles)}")
    print(f"Articles rejected: {rejected_count}")
    pass_rate = len(selected_articles)/len(unique_articles)*100 if unique_articles else 0
    print(f"Pass rate: {pass_rate:.1f}%")
    print()
    
    print(f"âœ… RSS feed generated: {output_file}")
    print("="*60)

if __name__ == "__main__":
    main()
