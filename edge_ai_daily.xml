<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>ğŸš€ Ultimate AI Feed - Authority First, Signal Over Noise</title><link>https://github.com/paramita619/hf-daily-paper-rss</link><description>Top 10 daily: semantic analysis, smart dedup, authority-first, quality over quantity.</description><lastBuildDate>Sun, 22 Feb 2026 23:14:34 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>[1] [å¹³å°åº•åº§] GGML and llama.cpp join HF to ensure the long-term progress of Local AI</title><link>https://huggingface.co/blog/ggml-joins-hf</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 144&lt;/strong&gt; | Category: å¹³å°åº•åº§&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; Hacker News | Domain: huggingface.co&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:Hacker News(+10)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:huggingface.co(+50)&lt;/li&gt;
&lt;li&gt;L1-Tech:ggml(+40)&lt;/li&gt;
&lt;li&gt;L1-Tech:local ai(+40)&lt;/li&gt;
&lt;li&gt;L1-Tech:llama.cpp(+40)&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Sun, 22 Feb 2026 23:14:34 GMT</pubDate></item><item><title>[2] [å¤§V/æƒå¨] Sam Altman would like to remind you that humans use a lot of energy, too</title><link>https://techcrunch.com/2026/02/21/sam-altman-would-like-remind-you-that-humans-use-a-lot-of-energy-too/</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 130&lt;/strong&gt; | Category: å¤§V/æƒå¨&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; TechCrunch | Domain: techcrunch.com&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:TechCrunch(+15)&lt;/li&gt;
&lt;li&gt;âœ“A-Domain:techcrunch.com(+30)&lt;/li&gt;
&lt;li&gt;ğŸ†S-Authority:sam altman(+100)&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.9&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“„ Summary:&lt;/strong&gt; "It also takes a lot of energy to train a human."...&lt;/p&gt;
&lt;/div&gt;</description><pubDate>Sun, 22 Feb 2026 23:14:34 GMT</pubDate></item><item><title>[3] [æ¨¡å‹ç®—æ³•] Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs</title><link>https://huggingface.co/papers/2602.10377</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 115&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; HF Papers | Domain: huggingface.co&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:HF Papers(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:huggingface.co(+50)&lt;/li&gt;
&lt;li&gt;L1-Tech:on-device(+40)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Sun, 22 Feb 2026 23:14:34 GMT</pubDate></item><item><title>[4] [å¹³å°åº•åº§] Apple researchers develop on-device AI agent that interacts with apps for you</title><link>https://9to5mac.com/2026/02/20/apple-researchers-develop-on-device-ai-agent-that-interacts-with-apps-for-you/</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 110&lt;/strong&gt; | Category: å¹³å°åº•åº§&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; Hacker News | Domain: 9to5mac.com&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:Hacker News(+10)&lt;/li&gt;
&lt;li&gt;L1-Tech:on-device ai(+40)&lt;/li&gt;
&lt;li&gt;L1-Tech:on-device(+40)&lt;/li&gt;
&lt;li&gt;âœ“Industry-Gate:pass(+20)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Sun, 22 Feb 2026 23:14:34 GMT</pubDate></item><item><title>[5] [æ¨¡å‹ç®—æ³•] Human-level 3D shape perception emerges from multi-view learning</title><link>http://arxiv.org/abs/2602.17650v1</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 108&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; AlphaXiv | Domain: arxiv.org&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:AlphaXiv(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:arxiv.org(+50)&lt;/li&gt;
&lt;li&gt;L1-Tech:npu(+40)&lt;/li&gt;
&lt;li&gt;âœ“Industry-Gate:pass(+20)&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“„ Summary:&lt;/strong&gt; Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape infe...&lt;/p&gt;
&lt;/div&gt;</description><pubDate>Sun, 22 Feb 2026 23:14:34 GMT</pubDate></item><item><title>[6] [æ¨¡å‹ç®—æ³•] Adaptive Decentralized Composite Optimization via Three-Operator Splitting</title><link>http://arxiv.org/abs/2602.17545v1</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 108&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; AlphaXiv | Domain: arxiv.org&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:AlphaXiv(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:arxiv.org(+50)&lt;/li&gt;
&lt;li&gt;edge_optimization&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“„ Summary:&lt;/strong&gt; The paper studies decentralized optimization over networks, where agents minimize a sum of {\it locally} smooth (strongly) convex losses and plus a nonsmooth convex extended value term. We propose decentralized methods wherein agents {\it adaptively} adjust their stepsize via local backtracking procedures coupled with lightweight min-consensus prot...&lt;/p&gt;
&lt;/div&gt;</description><pubDate>Sun, 22 Feb 2026 23:14:34 GMT</pubDate></item><item><title>[7] [æ¨¡å‹ç®—æ³•] DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers</title><link>https://huggingface.co/papers/2602.16968</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 105&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; HF Papers | Domain: huggingface.co&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:HF Papers(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:huggingface.co(+50)&lt;/li&gt;
&lt;li&gt;L3-Tech:transformer(+15)&lt;/li&gt;
&lt;li&gt;L3-Tech:diffusion(+15)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Sun, 22 Feb 2026 23:14:34 GMT</pubDate></item><item><title>[8] [å¹³å°åº•åº§] 9x MobileNet V2 size reduction with Quantization aware training</title><link>https://github.com/dakshjain-1616/Quantisation-Awareness-training-by-NEO</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 100&lt;/strong&gt; | Category: å¹³å°åº•åº§&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; Hacker News | Domain: github.com&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:Hacker News(+10)&lt;/li&gt;
&lt;li&gt;âœ“A-Domain:github.com(+30)&lt;/li&gt;
&lt;li&gt;L2-Tech:quantization(+25)&lt;/li&gt;
&lt;li&gt;edge_optimization&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Sun, 22 Feb 2026 23:14:34 GMT</pubDate></item><item><title>[9] [æ¨¡å‹ç®—æ³•] SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning</title><link>https://huggingface.co/papers/2602.13515</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 100&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; HF Papers | Domain: huggingface.co&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:HF Papers(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:huggingface.co(+50)&lt;/li&gt;
&lt;li&gt;L2-Tech:distillation(+25)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Sun, 22 Feb 2026 23:14:34 GMT</pubDate></item><item><title>[10] [æ¨¡å‹ç®—æ³•] CORAL: Correspondence Alignment for Improved Virtual Try-On</title><link>http://arxiv.org/abs/2602.17636v1</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 100&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; AlphaXiv | Domain: arxiv.org&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:AlphaXiv(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:arxiv.org(+50)&lt;/li&gt;
&lt;li&gt;L3-Tech:transformer(+15)&lt;/li&gt;
&lt;li&gt;L3-Tech:diffusion(+15)&lt;/li&gt;
&lt;li&gt;âœ“Industry-Gate:pass(+20)&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“„ Summary:&lt;/strong&gt; Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we ...&lt;/p&gt;
&lt;/div&gt;</description><pubDate>Sun, 22 Feb 2026 23:14:34 GMT</pubDate></item></channel></rss>