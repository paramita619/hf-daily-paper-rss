<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>ğŸš€ Ultimate AI Feed - Authority First, Signal Over Noise</title><link>https://github.com/paramita619/hf-daily-paper-rss</link><description>Top 10 daily: semantic analysis, smart dedup, authority-first, quality over quantity.</description><lastBuildDate>Mon, 09 Feb 2026 23:27:17 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>[1] [æ¨¡å‹ç®—æ³•] NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices</title><link>http://arxiv.org/abs/2602.06879v1</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 172&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; AlphaXiv | Domain: arxiv.org&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:AlphaXiv(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:arxiv.org(+50)&lt;/li&gt;
&lt;li&gt;L1-Tech:on-device(+40)&lt;/li&gt;
&lt;li&gt;L2-Tech:distillation(+25)&lt;/li&gt;
&lt;li&gt;L3-Tech:diffusion(+15)&lt;/li&gt;
&lt;li&gt;edge_optimization&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“„ Summary:&lt;/strong&gt; While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline de...&lt;/p&gt;
&lt;/div&gt;</description><pubDate>Mon, 09 Feb 2026 23:27:17 GMT</pubDate></item><item><title>[2] [å¹³å°åº•åº§] Show HN: Free On-Device AI SDK to Run PyTorch on Mobile NPUs (Open Source)</title><link>https://github.com/zetic-ai/ZETIC_MLange_apps</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 143&lt;/strong&gt; | Category: å¹³å°åº•åº§&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; Hacker News | Domain: github.com&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:Hacker News(+10)&lt;/li&gt;
&lt;li&gt;âœ“A-Domain:github.com(+30)&lt;/li&gt;
&lt;li&gt;L1-Tech:on-device ai(+40)&lt;/li&gt;
&lt;li&gt;L1-Tech:npu(+40)&lt;/li&gt;
&lt;li&gt;L1-Tech:on-device(+40)&lt;/li&gt;
&lt;li&gt;opensource_tool&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.7&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Mon, 09 Feb 2026 23:27:17 GMT</pubDate></item><item><title>[3] [å¤§V/æƒå¨] From Svedka to Anthropic, brands make bold plays with AI in Super Bowl ads</title><link>https://techcrunch.com/2026/02/08/super-bowl-60-ai-ads-svedka-anthropic-brands-commercials/</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 139&lt;/strong&gt; | Category: å¤§V/æƒå¨&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; TechCrunch | Domain: techcrunch.com&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:TechCrunch(+15)&lt;/li&gt;
&lt;li&gt;âœ“A-Domain:techcrunch.com(+30)&lt;/li&gt;
&lt;li&gt;â­A-Authority:openai(+70)&lt;/li&gt;
&lt;li&gt;â­A-Authority:anthropic(+70)&lt;/li&gt;
&lt;li&gt;âŒIndustry-Gate:fail(-30)&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.9&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“„ Summary:&lt;/strong&gt; From the first AI-generated Big Game ad courtesy of Svedka to Anthropic's beef with OpenAI, here are the biggest ads from Super Bowl LX....&lt;/p&gt;
&lt;/div&gt;</description><pubDate>Mon, 09 Feb 2026 23:27:17 GMT</pubDate></item><item><title>[4] [å¹³å°åº•åº§] Open source real-time screen analysis tool powered by Screenpipe and local LLM</title><link>https://github.com/cyrus-cai/livepipe</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 125&lt;/strong&gt; | Category: å¹³å°åº•åº§&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; Hacker News | Domain: github.com&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:Hacker News(+10)&lt;/li&gt;
&lt;li&gt;âœ“A-Domain:github.com(+30)&lt;/li&gt;
&lt;li&gt;L1-Tech:local llm(+40)&lt;/li&gt;
&lt;li&gt;opensource_tool&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Mon, 09 Feb 2026 23:27:17 GMT</pubDate></item><item><title>[5] [æ¨¡å‹ç®—æ³•] Vision Transformer Finetuning Benefits from Non-Smooth Components</title><link>http://arxiv.org/abs/2602.06883v1</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 116&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; AlphaXiv | Domain: arxiv.org&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:AlphaXiv(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:arxiv.org(+50)&lt;/li&gt;
&lt;li&gt;L1-Tech:npu(+40)&lt;/li&gt;
&lt;li&gt;L3-Tech:rag(+15)&lt;/li&gt;
&lt;li&gt;L3-Tech:transformer(+15)&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“„ Summary:&lt;/strong&gt; The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in oth...&lt;/p&gt;
&lt;/div&gt;</description><pubDate>Mon, 09 Feb 2026 23:27:17 GMT</pubDate></item><item><title>[6] [å¹³å°åº•åº§] Executorch: On-device AI across mobile, embedded and edge for PyTorch</title><link>https://github.com/pytorch/executorch</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 112&lt;/strong&gt; | Category: å¹³å°åº•åº§&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; Hacker News | Domain: github.com&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:Hacker News(+10)&lt;/li&gt;
&lt;li&gt;âœ“A-Domain:github.com(+30)&lt;/li&gt;
&lt;li&gt;L1-Tech:executorch(+40)&lt;/li&gt;
&lt;li&gt;L1-Tech:on-device ai(+40)&lt;/li&gt;
&lt;li&gt;L1-Tech:on-device(+40)&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.7&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Mon, 09 Feb 2026 23:27:17 GMT</pubDate></item><item><title>[7] [æ¨¡å‹ç®—æ³•] RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing</title><link>http://arxiv.org/abs/2602.06871v1</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 104&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; AlphaXiv | Domain: arxiv.org&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:AlphaXiv(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:arxiv.org(+50)&lt;/li&gt;
&lt;li&gt;L1-Tech:npu(+40)&lt;/li&gt;
&lt;li&gt;L3-Tech:diffusion(+15)&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“„ Summary:&lt;/strong&gt; Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video...&lt;/p&gt;
&lt;/div&gt;</description><pubDate>Mon, 09 Feb 2026 23:27:17 GMT</pubDate></item><item><title>[8] [æ¨¡å‹ç®—æ³•] Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing</title><link>http://arxiv.org/abs/2602.06862v1</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 104&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; AlphaXiv | Domain: arxiv.org&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:AlphaXiv(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:arxiv.org(+50)&lt;/li&gt;
&lt;li&gt;L1-Tech:npu(+40)&lt;/li&gt;
&lt;li&gt;L3-Tech:peft(+15)&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“„ Summary:&lt;/strong&gt; Adapting pre-trained vision models using parameter-efficient fine-tuning (PEFT) remains challenging, as it aims to achieve performance comparable to full fine-tuning using a minimal number of trainable parameters. When applied to complex dense prediction tasks, existing methods exhibit limitations, including input-agnostic modeling and redundant cr...&lt;/p&gt;
&lt;/div&gt;</description><pubDate>Mon, 09 Feb 2026 23:27:17 GMT</pubDate></item><item><title>[9] [æ¨¡å‹ç®—æ³•] QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals</title><link>https://huggingface.co/papers/2602.02581</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 100&lt;/strong&gt; | Category: æ¨¡å‹ç®—æ³•&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; HF Papers | Domain: huggingface.co&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:HF Papers(+25)&lt;/li&gt;
&lt;li&gt;ğŸ”’S-Domain:huggingface.co(+50)&lt;/li&gt;
&lt;li&gt;L2-Tech:quantization(+25)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Mon, 09 Feb 2026 23:27:17 GMT</pubDate></item><item><title>[10] [å¹³å°åº•åº§] LlamaLib: A cross-platform C++/C# library for local LLMs based on llama.cpp</title><link>https://github.com/undreamai/LlamaLib</link><description>&lt;div style='font-family: sans-serif; padding: 10px;'&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“Š Score: 96&lt;/strong&gt; | Category: å¹³å°åº•åº§&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ğŸ“¡ Source:&lt;/strong&gt; Hacker News | Domain: github.com&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;âœ¨ Why selected:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source:Hacker News(+10)&lt;/li&gt;
&lt;li&gt;âœ“A-Domain:github.com(+30)&lt;/li&gt;
&lt;li&gt;L1-Tech:local llm(+40)&lt;/li&gt;
&lt;li&gt;L1-Tech:llama.cpp(+40)&lt;/li&gt;
&lt;li&gt;TimeFactor:Ã—0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description><pubDate>Mon, 09 Feb 2026 23:27:17 GMT</pubDate></item></channel></rss>